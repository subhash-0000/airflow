 .. Licensed to the Apache Software Foundation (ASF) under one
    or more contributor license agreements.  See the NOTICE file
    distributed with this work for additional information
    regarding copyright ownership.  The ASF licenses this file
    to you under the Apache License, Version 2.0 (the
    "License"); you may not use this file except in compliance
    with the License.  You may obtain a copy of the License at

 ..   http://www.apache.org/licenses/LICENSE-2.0

 .. Unless required by applicable law or agreed to in writing,
    software distributed under the License is distributed on an
    "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
    KIND, either express or implied.  See the License for the
    specific language governing permissions and limitations
    under the License.


.. _write-logs-advanced:

Advanced logging configuration
==============================

Not all configuration options are available from the ``airflow.cfg`` file. The config file describes
how to configure logging for tasks, because the logs generated by tasks are not only logged in separate
files by default but has to be also accessible via the webserver.

By default standard Airflow component logs are written to the ``$AIRFLOW_HOME/logs`` directory, but you
can also customize it and configure it as you want by overriding Python logger configuration that can
be configured by providing custom logging configuration object. You can also create and use logging configuration
for specific operators and tasks.

Some configuration options require that the logging config class be overwritten. You can do it by copying the default
configuration of Airflow and modifying it to suit your needs.

The default configuration can be seen in the
`airflow_local_settings.py template <https://github.com/apache/airflow/blob/|airflow-version|/airflow-core/src/airflow/config_templates/airflow_local_settings.py>`_
and you can see the loggers and handlers used there.

See :ref:`Configuring local settings <set-config:configuring-local-settings>` for details on how to
configure local settings.

Except the custom loggers and handlers configurable there via the ``airflow.cfg``, the logging methods in Airflow follow the usual Python logging convention,
that Python objects log to loggers that follow naming convention of ``<package>.<module_name>``.

You can read more about standard python logging classes (Loggers, Handlers, Formatters) in the
`Python logging documentation <https://docs.python.org/library/logging.html>`_.

Create a custom logging class
-----------------------------


.. important::

   **Migration Note: Logging in Airflow 3.x and Later**

   Airflow 3.0 and later uses ``structlog`` for all logging configuration. The legacy
   ``DEFAULT_LOGGING_CONFIG`` and ``dictConfig``-based customization are no longer respected.
   To customize logging, please refer to the ``structlog``-based configuration patterns below. If you have existing custom logging code or configuration using
   ``DEFAULT_LOGGING_CONFIG``, you must migrate to ``structlog``. See the migration guide for details.


Customizing logging in Airflow 3.x+ is done via the ``logging_config_class`` option in ``airflow.cfg`` file, but the configuration must use ``structlog`` patterns.

**Simplest way to enable JSON logging:**

.. code-block:: bash

   export AIRFLOW__LOGGING__JSON_FORMAT=True

Or set it in your environment or Dockerfile.

**Advanced: Custom ``structlog`` configuration**

Create a file (e.g., ``~/airflow/config/log_config.py``) with the following contents:

.. code-block:: python

   import structlog
   from airflow.config_templates.airflow_local_settings import DEFAULT_LOGGING_CONFIG
   from copy import ``deepcopy``

   json_processors = [
      structlog.processors.TimeStamper(fmt="iso"),
      structlog.stdlib.add_log_level,
      structlog.processors.StackInfoRenderer(),
      structlog.processors.format_exc_info,
      structlog.processors.UnicodeDecoder(),
      structlog.processors.JSONRenderer(),
   ]

   LOGGING_CONFIG = ``deepcopy``(DEFAULT_LOGGING_CONFIG)
   LOGGING_CONFIG["structlog"] = {
      "processors": json_processors,
      "wrapper_class": structlog.stdlib.BoundLogger,
      "logger_factory": structlog.stdlib.LoggerFactory(),
      "cache_logger_on_first_use": True,
   }

Then in your ``airflow.cfg``:

.. code-block:: ini

   [logging]
   logging_config_class = log_config.LOGGING_CONFIG

You can further customize the processor chain or handlers as needed. For more advanced ``structlog`` configuration, see the `structlog documentation <https://www.structlog.org/en/stable/>`_.

Restart the application after making changes.

See :doc:`../modules_management` for details on how Python and Airflow manage modules.


.. note::

   You can override the way both standard logs of the components and "task" logs are handled.


Custom logger for Operators, Hooks and Tasks
--------------------------------------------

You can create custom logging handlers and apply them to specific Operators, Hooks and tasks. By default, the Operators
and Hooks loggers are child of the ``airflow.task`` logger: They follow respectively the naming convention
``airflow.task.operators.<package>.<module_name>`` and ``airflow.task.hooks.<package>.<module_name>``. After
:doc:`creating a custom logging class </administration-and-deployment/logging-monitoring/advanced-logging-configuration>`,
you can assign specific loggers to them.


.. note::

   The following examples are for Airflow 2.x and earlier. For Airflow 3.x+, use ``structlog``-based configuration as shown above.


You can also set a custom name to a Dag's task with the ``logger_name`` attribute. This can be useful if multiple tasks
are using the same Operator, but you want to disable logging for some of them.

Example of custom logger name:

    .. code-block:: python

      # In your Dag file
      SQLExecuteQueryOperator(..., logger_name="sql.big_query")


            # In your custom `log_config.py` (Airflow 2.x and earlier only)
            # For Airflow 3.x+, use ``structlog`` as shown above.

If you want to limit the log size of the tasks, you can add the handlers.task.max_bytes parameter.

Example of limiting the size of tasks:

    .. code-block:: python


            # In your custom `log_config.py` (Airflow 2.x and earlier only)
            # For Airflow 3.x+, use ``structlog`` as shown above.
